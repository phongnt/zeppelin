name: core

on: workflow_dispatch

env:
  # Disable keepAlive and pool
  # https://github.com/actions/virtual-environments/issues/1499#issuecomment-689467080
  MAVEN_OPTS: >-
      -Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit
      -Dhttp.keepAlive=false
      -Dmaven.wagon.http.pool=false
      -Dmaven.wagon.http.retryHandler.count=3
  MAVEN_ARGS: >-
      -B --no-transfer-progress
  ZEPPELIN_HELIUM_REGISTRY: helium
  SPARK_PRINT_LAUNCH_COMMAND: "true"
  SPARK_LOCAL_IP: 127.0.0.1
  ZEPPELIN_LOCAL_IP: 127.0.0.1

# Use the bash login, because we are using miniconda
defaults:
  run:
    shell: bash -l {0}

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  # test on spark for each spark version & scala version
  spark-test:
    runs-on: ubuntu-20.04
    strategy:
      fail-fast: false
      matrix:
        python: [ 3.7 ]
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Tune Runner VM
        uses: ./.github/actions/tune-runner-vm
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: 8
      - name: Cache local Maven repository
        uses: actions/cache@v3
        with:
          path: |
            ~/.m2/repository
            !~/.m2/repository/org/apache/zeppelin/
            ~/.spark-dist
            ~/.cache
          key: ${{ runner.os }}-zeppelin-dfb3df3ec17d1e28875b12b33e4d519c63843356b3e9a90518f996ff0ec46887
          restore-keys: |
            ${{ runner.os }}-zeppelin-
      - name: install environment
        run: ./mvnw install -DskipTests -pl spark-submit,spark/scala-2.12 -am -Phadoop2 ${MAVEN_ARGS}
      - name: Setup conda environment with python ${{ matrix.python }} and R
        uses: conda-incubator/setup-miniconda@v2
        with:
          activate-environment: python_3_with_R
          environment-file: testing/env_python_${{ matrix.python }}_with_R.yml
          python-version: ${{ matrix.python }}
          miniforge-variant: Mambaforge
          channels: conda-forge,defaults
          channel-priority: true
          auto-activate-base: false
          use-mamba: true
      - name: Make IRkernel available to Jupyter
        run: |
          R -e "IRkernel::installspec()"
      - name: run spark-3.2 tests with scala-2.12 and python-${{ matrix.python }}
        run: |
          rm -rf spark/interpreter/metastore_db
          ./mvnw verify -pl spark/interpreter -am -Dtest=SparkIRInterpreterTest,SparkRInterpreterTest,IPySparkInterpreterTest -Pspark-3.2 -Pspark-scala-2.12 -Phadoop2 -Pintegration -DskipShaded=true -DfailIfNoTests=false ${MAVEN_ARGS}
